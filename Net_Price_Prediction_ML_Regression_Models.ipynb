{
  "metadata": {
    "name": "Project_Practice_with_extra_columns",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Regression Model\n\n### Regression models are a class of statistical models used to predict a continuous target variable based on one or more predictor variables."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Import Spark SQL and Spark ML Libraries\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# from pyspark.ml.feature import VectorAssembler\n\nfrom pyspark.sql.functions import mean, col, when\n\nfrom pyspark.ml.regression import RandomForestRegressor,GBTRegressor, DecisionTreeRegressor, LinearRegression\nfrom pyspark.sql.functions import *\nfrom pyspark.ml.feature import *\nfrom pyspark.ml import *\nfrom pyspark.ml.evaluation import *\nfrom pyspark.mllib.evaluation import *\nfrom pyspark.sql.types import *\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator,TrainValidationSplit\n\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession\n\nimport time"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nIS_SPARK_SUBMIT_CLI \u003d False\nif IS_SPARK_SUBMIT_CLI:\n    sc \u003d SparkContext.getOrCreate()\n    spark \u003d SparkSession(sc)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Limit the log\nspark.sparkContext.setLogLevel(\"WARN\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Loading data from a CSV file into a Spark DataFrame."
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# File location and type\nfile_location \u003d \"/user/aporwal/Project/export.csv\"\nfile_type \u003d \"csv\"\n\n# CSV options\ninfer_schema \u003d \"true\"\nfirst_row_is_header \u003d \"true\"\ndelimiter \u003d \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf \u003d spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndf.show(10)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Performing data cleaning and preprocessing tasks such as handling missing values, casting data types, and creating new features. "
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n\r\nfrom pyspark.sql.functions import col, when\r\n\r\ncolumns \u003d [\u0027UNITID\u0027,\u0027INSTNM\u0027,\u0027STABBR\u0027 , \u0027LOCALE\u0027, \u0027CONTROL\u0027, \u0027HBCU\u0027, \u0027PBI\u0027,\r\n           \u0027ANNHI\u0027, \u0027TRIBAL\u0027, \u0027AANAPII\u0027, \u0027HSI\u0027, \u0027NANTI\u0027,\u0027SATVRMID\u0027, \u0027SATMTMID\u0027 ,\u0027ACTCMMID\u0027, \u0027ACTENMID\u0027, \r\n           \u0027ACTMTMID\u0027  , \u0027SAT_AVG\u0027,\u0027SAT_AVG_ALL\u0027  , \u0027MD_EARN_WNE_P10\u0027 , \u0027GT_25K_P6\u0027 , \u0027GRAD_DEBT_MDN_SUPP\u0027\r\n            , \u0027RPY_3YR_RT_SUPP\u0027, \u0027NPT4_PUB\u0027, \u0027NPT4_PRIV\u0027,\u0027COSTT4_A\u0027,\u0027UGDS\u0027,\u0027TUITIONFEE_IN\u0027,\u0027TUITIONFEE_OUT\u0027,\u0027PCTPELL\u0027,\u0027LPSTAFFORD_CNT\u0027,\u0027LPSTAFFORD_AMT\u0027,\u0027LPPPLUS_CNT\u0027,\u0027LPPPLUS_AMT\u0027,\u0027BOOKSUPPLY\u0027,\u0027ROOMBOARD_ON\u0027,\u0027OTHEREXPENSE_ON\u0027,\u0027ROOMBOARD_OFF\u0027,\u0027OTHEREXPENSE_OFF\u0027,\u0027OTHEREXPENSE_FAM\u0027,\u0027ENDOWBEGIN\u0027,\u0027ENDOWEND\u0027,\u0027ADM_RATE\u0027,\u0027ENRL_ORIG_YR2_RT\u0027,\u0027AGE_ENTRY\u0027 ,\u0027UGDS_MEN\u0027,\u0027UGDS_WOMEN\u0027]\r\n           \r\n# Create a new DataFrame dfp with selected columns\r\ndfp \u003d df.select(*columns)\r\n\r\n#Casting values to float\r\ndfp \u003d dfp.withColumn(\u0027NPT4_PUB\u0027, col(\u0027NPT4_PUB\u0027).cast(\"float\"))  \r\ndfp \u003d dfp.withColumn(\u0027NPT4_PRIV\u0027, col(\u0027NPT4_PRIV\u0027).cast(\"float\"))\r\n\r\n# Fill missing values with 0 for NPT4_PUB and NPT4_PRIV columns\r\ndfp \u003d dfp.withColumn(\u0027NPT4_PUB\u0027, when(col(\u0027NPT4_PUB\u0027).isNull(), 0).otherwise(col(\u0027NPT4_PUB\u0027)))\r\ndfp \u003d dfp.withColumn(\u0027NPT4_PRIV\u0027, when(col(\u0027NPT4_PRIV\u0027).isNull(), 0).otherwise(col(\u0027NPT4_PRIV\u0027)))\r\n \r\n# Calculate Net_Price by summing NPT4_PUB and NPT4_PRIV\r\ndfp \u003d dfp.withColumn(\u0027Net_Price\u0027, col(\u0027NPT4_PUB\u0027) + col(\u0027NPT4_PRIV\u0027))\r\n \r\n# Remove outliers and replace with NaN\r\ndfp \u003d dfp.withColumn(\u0027Net_Price\u0027, when((col(\u0027Net_Price\u0027) \u003c 1) | (col(\u0027Net_Price\u0027) \u003e 55000), None).otherwise(col(\u0027Net_Price\u0027)))\r\n "
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n#Data cleaning\n\n# Drop rows containing null values\ndfp \u003d dfp.na.drop()\n\n#Drop unwanted columns since we have calculated Net_price\ndfp \u003d dfp.drop(\u0027NPT4_PUB\u0027, \u0027NPT4_PRIV\u0027)\n\n#remove \"PrivacySupressed\" values and replace it with null\n\nclean_columns \u003d [\u0027MD_EARN_WNE_P10\u0027, \u0027GT_25K_P6\u0027, \u0027GRAD_DEBT_MDN_SUPP\u0027,\u0027RPY_3YR_RT_SUPP\u0027,\u0027LPSTAFFORD_CNT\u0027,\u0027LPSTAFFORD_AMT\u0027,\u0027LPPPLUS_CNT\u0027,\u0027LPPPLUS_AMT\u0027,\u0027ADM_RATE\u0027,\u0027ENRL_ORIG_YR2_RT\u0027,\u0027AGE_ENTRY\u0027,\u0027UGDS_MEN\u0027,\u0027UGDS_WOMEN\u0027]\n\nfor column in clean_columns:\n    dfp \u003d dfp.withColumn(column, when(dfp[column].isin([\u0027PrivacySuppressed\u0027]), None).otherwise(dfp[column].cast(\"float\")))\n\n# Show the updated DataFrame\ndfp.show(2)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Feature Engineering to create new columns as features\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\u0027\u0027\u0027\nFeature engineering , Create a new column represent the SAT score and drop the columns ( \u0027SATVRMID\u0027 , \u0027SATMTMID\u0027)\nFeature engineering , Create a new column represent the Average SAT score and drop the columns ( \u0027SAT_AVG\u0027 , \u0027SAT_AVG_ALL\u0027)\nFeature engineering , Create a new column represent the ACT score and drop the columns ( \u0027ACTCMMID\u0027 , \u0027ACTENMID\u0027 , \u0027ACTMTMID\u0027)\n\u0027\u0027\u0027\n#SAT_Score\ndfp \u003d dfp.withColumn(\u0027SATVRMID\u0027, col(\u0027SATVRMID\u0027).cast(\"float\"))  \ndfp \u003d dfp.withColumn(\u0027SATMTMID\u0027, col(\u0027SATMTMID\u0027).cast(\"float\"))  \ndfp \u003d dfp.withColumn(\u0027SATVRMID\u0027, when(col(\u0027SATVRMID\u0027).isNull(), 0).otherwise(col(\u0027SATVRMID\u0027)))\ndfp \u003d dfp.withColumn(\u0027SATMTMID\u0027, when(col(\u0027SATMTMID\u0027).isNull(), 0).otherwise(col(\u0027SATMTMID\u0027)))\ndfp \u003d dfp.withColumn(\u0027SAT_Score\u0027, col(\u0027SATVRMID\u0027) + col(\u0027SATMTMID\u0027))\n\n#Average_SAT\ndfp \u003d dfp.withColumn(\u0027SAT_AVG\u0027, col(\u0027SAT_AVG\u0027).cast(\"float\"))  \ndfp \u003d dfp.withColumn(\u0027SAT_AVG_ALL\u0027, col(\u0027SAT_AVG_ALL\u0027).cast(\"float\"))  \ndfp \u003d dfp.withColumn(\u0027SAT_AVG\u0027, when(col(\u0027SAT_AVG\u0027).isNull(), 0).otherwise(col(\u0027SAT_AVG\u0027)))\ndfp \u003d dfp.withColumn(\u0027SAT_AVG_ALL\u0027, when(col(\u0027SAT_AVG_ALL\u0027).isNull(), 0).otherwise(col(\u0027SAT_AVG_ALL\u0027)))\ndfp \u003d dfp.withColumn(\u0027Average_SAT\u0027, col(\u0027SAT_AVG\u0027) + col(\u0027SAT_AVG_ALL\u0027))\n\n#ACT_Score\ndfp \u003d dfp.withColumn(\u0027ACTCMMID\u0027, col(\u0027ACTCMMID\u0027).cast(\"float\"))  \ndfp \u003d dfp.withColumn(\u0027ACTENMID\u0027, col(\u0027ACTENMID\u0027).cast(\"float\"))\ndfp \u003d dfp.withColumn(\u0027ACTMTMID\u0027, col(\u0027ACTMTMID\u0027).cast(\"float\"))  \ndfp \u003d dfp.withColumn(\u0027ACTCMMID\u0027, when(col(\u0027ACTCMMID\u0027).isNull(), 0).otherwise(col(\u0027ACTCMMID\u0027)))\ndfp \u003d dfp.withColumn(\u0027ACTENMID\u0027, when(col(\u0027ACTENMID\u0027).isNull(), 0).otherwise(col(\u0027ACTENMID\u0027)))\ndfp \u003d dfp.withColumn(\u0027ACTMTMID\u0027, when(col(\u0027ACTMTMID\u0027).isNull(), 0).otherwise(col(\u0027ACTMTMID\u0027)))\ndfp \u003d dfp.withColumn(\u0027ACT_Score\u0027, col(\u0027ACTCMMID\u0027) + col(\u0027ACTENMID\u0027) + col(\u0027ACTMTMID\u0027))\n\n\n#Check if Earning , Aid and repayment columns are null, if yes, then replace it with the mean value\n\nmean_MD_EARN_WNE_P10 \u003d dfp.select(mean(col(\"MD_EARN_WNE_P10\"))).collect()[0][0]\nmean_GT_25K_P6 \u003d dfp.select(mean(col(\"GT_25K_P6\"))).collect()[0][0]\nmean_GRAD_DEBT_MDN_SUPP \u003d dfp.select(mean(col(\"GRAD_DEBT_MDN_SUPP\"))).collect()[0][0]\nmean_RPY_3YR_RT_SUPP \u003d dfp.select(mean(col(\"RPY_3YR_RT_SUPP\"))).collect()[0][0]\n\n# Replace null values with mean values\ndfp \u003d dfp.withColumn(\"MD_EARN_WNE_P10\", when(col(\"MD_EARN_WNE_P10\").isNull(), mean_MD_EARN_WNE_P10).otherwise(col(\"MD_EARN_WNE_P10\")))\ndfp \u003d dfp.withColumn(\"GT_25K_P6\", when(col(\"GT_25K_P6\").isNull(), mean_GT_25K_P6).otherwise(col(\"GT_25K_P6\")))\ndfp \u003d dfp.withColumn(\"GRAD_DEBT_MDN_SUPP\", when(col(\"GRAD_DEBT_MDN_SUPP\").isNull(), mean_GRAD_DEBT_MDN_SUPP).otherwise(col(\"GRAD_DEBT_MDN_SUPP\")))\ndfp \u003d dfp.withColumn(\"RPY_3YR_RT_SUPP\", when(col(\"RPY_3YR_RT_SUPP\").isNull(), mean_RPY_3YR_RT_SUPP).otherwise(col(\"RPY_3YR_RT_SUPP\")))\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#COSTT4_A \ndfp \u003d dfp.withColumn(\u0027COSTT4_A\u0027, col(\u0027COSTT4_A\u0027).cast(\"float\"))\n\n#UGDS\ndfp \u003d dfp.withColumn(\u0027UGDS\u0027, col(\u0027UGDS\u0027).cast(\"float\"))\n\n#loan, tuition and expenses\ndfp \u003d dfp.withColumn(\u0027TUITIONFEE_IN\u0027, col(\u0027TUITIONFEE_IN\u0027).cast(\"float\"))\ndfp \u003d dfp.withColumn(\u0027TUITIONFEE_OUT\u0027, col(\u0027TUITIONFEE_OUT\u0027).cast(\"float\"))\ndfp \u003d dfp.withColumn(\u0027PCTPELL\u0027, col(\u0027PCTPELL\u0027).cast(\"float\"))\ndfp \u003d dfp.withColumn(\u0027LPSTAFFORD_CNT\u0027, col(\u0027LPSTAFFORD_CNT\u0027).cast(\"float\"))\ndfp \u003d dfp.withColumn(\u0027LPSTAFFORD_AMT\u0027, col(\u0027LPSTAFFORD_AMT\u0027).cast(\"float\"))\ndfp \u003d dfp.withColumn(\u0027LPPPLUS_CNT\u0027, col(\u0027LPPPLUS_CNT\u0027).cast(\"float\"))\ndfp \u003d dfp.withColumn(\u0027LPPPLUS_AMT\u0027, col(\u0027LPPPLUS_AMT\u0027).cast(\"float\"))\ndfp \u003d dfp.withColumn(\u0027BOOKSUPPLY\u0027, col(\u0027BOOKSUPPLY\u0027).cast(\"float\"))\ndfp \u003d dfp.withColumn(\u0027ROOMBOARD_ON\u0027, col(\u0027ROOMBOARD_ON\u0027).cast(\"float\"))\ndfp \u003d dfp.withColumn(\u0027OTHEREXPENSE_ON\u0027, col(\u0027OTHEREXPENSE_ON\u0027).cast(\"float\"))\ndfp \u003d dfp.withColumn(\u0027ROOMBOARD_OFF\u0027, col(\u0027ROOMBOARD_OFF\u0027).cast(\"float\"))\ndfp \u003d dfp.withColumn(\u0027OTHEREXPENSE_OFF\u0027, col(\u0027OTHEREXPENSE_OFF\u0027).cast(\"float\"))\ndfp \u003d dfp.withColumn(\u0027OTHEREXPENSE_FAM\u0027, col(\u0027OTHEREXPENSE_FAM\u0027).cast(\"float\"))\ndfp \u003d dfp.withColumn(\u0027ENDOWBEGIN\u0027, col(\u0027ENDOWBEGIN\u0027).cast(\"float\"))\ndfp \u003d dfp.withColumn(\u0027ENDOWEND\u0027, col(\u0027ENDOWEND\u0027).cast(\"float\"))\n\n#student demographics, addmission_rate, age, gender\n\ndfp \u003d dfp.withColumn(\u0027ADM_RATE\u0027, col(\u0027ADM_RATE\u0027).cast(\"float\"))\ndfp \u003d dfp.withColumn(\u0027ENRL_ORIG_YR2_RT\u0027, col(\u0027ENRL_ORIG_YR2_RT\u0027).cast(\"float\"))\ndfp \u003d dfp.withColumn(\u0027AGE_ENTRY\u0027, col(\u0027AGE_ENTRY\u0027).cast(\"float\"))\ndfp \u003d dfp.withColumn(\u0027UGDS_MEN\u0027, col(\u0027UGDS_MEN\u0027).cast(\"float\"))\ndfp \u003d dfp.withColumn(\u0027UGDS_WOMEN\u0027, col(\u0027UGDS_WOMEN\u0027).cast(\"float\"))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Calculating mean values and replacing null values with mean values"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#replace null and 0\u0027s with the mean value\n\nmean_COSTT4_A \u003d dfp.select(mean(col(\"COSTT4_A\"))).collect()[0][0]\nmean_TUITIONFEE_IN \u003d dfp.select(mean(col(\"TUITIONFEE_IN\"))).collect()[0][0]\nmean_TUITIONFEE_OUT \u003d dfp.select(mean(col(\"TUITIONFEE_OUT\"))).collect()[0][0]\nmean_PCTPELL \u003d dfp.select(mean(col(\"PCTPELL\"))).collect()[0][0]\nmean_LPSTAFFORD_CNT \u003d dfp.select(mean(col(\"LPSTAFFORD_CNT\"))).collect()[0][0]\nmean_LPSTAFFORD_AMT \u003d dfp.select(mean(col(\"LPSTAFFORD_AMT\"))).collect()[0][0]\nmean_LPPPLUS_CNT \u003d dfp.select(mean(col(\"LPPPLUS_CNT\"))).collect()[0][0]\nmean_LPPPLUS_AMT \u003d dfp.select(mean(col(\"LPPPLUS_AMT\"))).collect()[0][0]\nmean_BOOKSUPPLY \u003d dfp.select(mean(col(\"BOOKSUPPLY\"))).collect()[0][0]\nmean_ROOMBOARD_ON \u003d dfp.select(mean(col(\"ROOMBOARD_ON\"))).collect()[0][0]\nmean_OTHEREXPENSE_ON \u003d dfp.select(mean(col(\"OTHEREXPENSE_ON\"))).collect()[0][0]\nmean_ROOMBOARD_OFF \u003d dfp.select(mean(col(\"ROOMBOARD_OFF\"))).collect()[0][0]\nmean_OTHEREXPENSE_OFF \u003d dfp.select(mean(col(\"OTHEREXPENSE_OFF\"))).collect()[0][0]\nmean_OTHEREXPENSE_FAM \u003d dfp.select(mean(col(\"OTHEREXPENSE_FAM\"))).collect()[0][0]\nmean_ENDOWBEGIN \u003d dfp.select(mean(col(\"ENDOWBEGIN\"))).collect()[0][0]\nmean_ENDOWEND \u003d dfp.select(mean(col(\"ENDOWEND\"))).collect()[0][0]\nmean_UGDS \u003d dfp.select(mean(col(\"UGDS\"))).collect()[0][0]\nmean_ADM_RATE \u003d dfp.select(mean(col(\"ADM_RATE\"))).collect()[0][0]\nmean_ENRL_ORIG_YR2_RT \u003d dfp.select(mean(col(\"ENRL_ORIG_YR2_RT\"))).collect()[0][0]\nmean_AGE_ENTRY \u003d dfp.select(mean(col(\"AGE_ENTRY\"))).collect()[0][0]\nmean_UGDS_MEN \u003d dfp.select(mean(col(\"UGDS_MEN\"))).collect()[0][0]\nmean_UGDS_WOMEN\u003d dfp.select(mean(col(\"UGDS_WOMEN\"))).collect()[0][0]\n\n\n\n# Replace null values with mean values\ndfp \u003d dfp.withColumn(\"COSTT4_A\", when((col(\"COSTT4_A\").isNull()) | (col(\"COSTT4_A\") \u003d\u003d 0.0), mean_COSTT4_A).otherwise(col(\"COSTT4_A\")))\ndfp \u003d dfp.withColumn(\"UGDS\", when((col(\"UGDS\").isNull()) | (col(\"UGDS\") \u003d\u003d 0.0), mean_UGDS).otherwise(col(\"UGDS\")))\ndfp \u003d dfp.withColumn(\"TUITIONFEE_IN\", when((col(\"TUITIONFEE_IN\").isNull()) | (col(\"TUITIONFEE_IN\") \u003d\u003d 0.0), mean_TUITIONFEE_IN).otherwise(col(\"TUITIONFEE_IN\")))\ndfp \u003d dfp.withColumn(\"TUITIONFEE_OUT\", when((col(\"TUITIONFEE_OUT\").isNull()) | (col(\"TUITIONFEE_OUT\") \u003d\u003d 0.0), mean_TUITIONFEE_OUT).otherwise(col(\"TUITIONFEE_OUT\")))\ndfp \u003d dfp.withColumn(\"PCTPELL\", when((col(\"PCTPELL\").isNull()) | (col(\"PCTPELL\") \u003d\u003d 0.0), mean_PCTPELL).otherwise(col(\"PCTPELL\")))\ndfp \u003d dfp.withColumn(\"LPSTAFFORD_CNT\", when((col(\"LPSTAFFORD_CNT\").isNull()) | (col(\"LPSTAFFORD_CNT\") \u003d\u003d 0.0), mean_LPSTAFFORD_CNT).otherwise(col(\"LPSTAFFORD_CNT\")))\ndfp \u003d dfp.withColumn(\"LPSTAFFORD_AMT\", when((col(\"LPSTAFFORD_AMT\").isNull()) | (col(\"LPSTAFFORD_AMT\") \u003d\u003d 0.0), mean_LPSTAFFORD_AMT).otherwise(col(\"LPSTAFFORD_AMT\")))\ndfp \u003d dfp.withColumn(\"LPPPLUS_CNT\", when((col(\"LPPPLUS_CNT\").isNull()) | (col(\"LPPPLUS_CNT\") \u003d\u003d 0.0), mean_LPPPLUS_CNT).otherwise(col(\"LPPPLUS_CNT\")))\ndfp \u003d dfp.withColumn(\"LPPPLUS_AMT\", when((col(\"LPPPLUS_AMT\").isNull()) | (col(\"LPPPLUS_AMT\") \u003d\u003d 0.0), mean_LPPPLUS_AMT).otherwise(col(\"LPPPLUS_AMT\")))\ndfp \u003d dfp.withColumn(\"BOOKSUPPLY\", when((col(\"BOOKSUPPLY\").isNull()) | (col(\"BOOKSUPPLY\") \u003d\u003d 0.0), mean_BOOKSUPPLY).otherwise(col(\"BOOKSUPPLY\")))\ndfp \u003d dfp.withColumn(\"ROOMBOARD_ON\", when((col(\"ROOMBOARD_ON\").isNull()) | (col(\"ROOMBOARD_ON\") \u003d\u003d 0.0), mean_ROOMBOARD_ON).otherwise(col(\"ROOMBOARD_ON\")))\ndfp \u003d dfp.withColumn(\"OTHEREXPENSE_ON\", when((col(\"OTHEREXPENSE_ON\").isNull()) | (col(\"OTHEREXPENSE_ON\") \u003d\u003d 0.0), mean_OTHEREXPENSE_ON).otherwise(col(\"OTHEREXPENSE_ON\")))\ndfp \u003d dfp.withColumn(\"ROOMBOARD_OFF\", when((col(\"ROOMBOARD_OFF\").isNull()) | (col(\"ROOMBOARD_OFF\") \u003d\u003d 0.0), mean_ROOMBOARD_OFF).otherwise(col(\"ROOMBOARD_OFF\")))\ndfp \u003d dfp.withColumn(\"OTHEREXPENSE_OFF\", when((col(\"OTHEREXPENSE_OFF\").isNull()) | (col(\"OTHEREXPENSE_OFF\") \u003d\u003d 0.0), mean_OTHEREXPENSE_OFF).otherwise(col(\"OTHEREXPENSE_OFF\")))\ndfp \u003d dfp.withColumn(\"OTHEREXPENSE_FAM\", when((col(\"OTHEREXPENSE_FAM\").isNull()) | (col(\"OTHEREXPENSE_FAM\") \u003d\u003d 0.0), mean_OTHEREXPENSE_FAM).otherwise(col(\"OTHEREXPENSE_FAM\")))\ndfp \u003d dfp.withColumn(\"ENDOWBEGIN\", when((col(\"ENDOWBEGIN\").isNull()) | (col(\"ENDOWBEGIN\") \u003d\u003d 0.0), mean_ENDOWBEGIN).otherwise(col(\"ENDOWBEGIN\")))\ndfp \u003d dfp.withColumn(\"ENDOWEND\", when((col(\"ENDOWEND\").isNull()) | (col(\"ENDOWEND\") \u003d\u003d 0.0), mean_ENDOWEND).otherwise(col(\"ENDOWEND\")))\ndfp \u003d dfp.withColumn(\"ADM_RATE\", when((col(\"ADM_RATE\").isNull()) | (col(\"ADM_RATE\") \u003d\u003d 0.0), mean_ADM_RATE).otherwise(col(\"ADM_RATE\")))\ndfp \u003d dfp.withColumn(\"ENRL_ORIG_YR2_RT\", when((col(\"ENRL_ORIG_YR2_RT\").isNull()) | (col(\"ENRL_ORIG_YR2_RT\") \u003d\u003d 0.0), mean_ENRL_ORIG_YR2_RT).otherwise(col(\"ENRL_ORIG_YR2_RT\")))\ndfp \u003d dfp.withColumn(\"AGE_ENTRY\", when((col(\"AGE_ENTRY\").isNull()) | (col(\"AGE_ENTRY\") \u003d\u003d 0.0), mean_AGE_ENTRY).otherwise(col(\"AGE_ENTRY\")))\ndfp \u003d dfp.withColumn(\"UGDS_MEN\", when((col(\"UGDS_MEN\").isNull()) | (col(\"UGDS_MEN\") \u003d\u003d 0.0), mean_UGDS_MEN).otherwise(col(\"UGDS_MEN\")))\ndfp \u003d dfp.withColumn(\"UGDS_WOMEN\", when((col(\"UGDS_WOMEN\").isNull()) | (col(\"UGDS_WOMEN\") \u003d\u003d 0.0), mean_UGDS_WOMEN).otherwise(col(\"UGDS_WOMEN\")))\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Visualize histograms to identify outliers in the data and then handle them by removing the extreme values"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Finding outliers for SAT_Score\nimport matplotlib.pyplot as plt\n\ndata \u003d dfp.select(\u0027SAT_Score\u0027).toPandas()\n\n# Plotting a histogram using Matplotlib\nplt.hist(data[\u0027SAT_Score\u0027], bins\u003d50)\nplt.title(\u0027Histogram of Values\u0027)\nplt.xlabel(\u0027SAT_Score\u0027)\nplt.ylabel(\u0027count\u0027)\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Finding outliers for Average_SAT\n\nimport matplotlib.pyplot as plt\n\ndata \u003d dfp.select(\u0027Average_SAT\u0027).toPandas()\n\n# Plotting a histogram using Matplotlib\nplt.hist(data[\u0027Average_SAT\u0027], bins\u003d70)\nplt.title(\u0027Histogram of Values\u0027)\nplt.xlabel(\u0027Average_SAT\u0027)\nplt.ylabel(\u0027count\u0027)\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Finding outliers for ACT_Score\n\nimport matplotlib.pyplot as plt\n\ndata \u003d dfp.select(\u0027ACT_Score\u0027).toPandas()\n\n# Plotting a histogram using Matplotlib\nplt.hist(data[\u0027ACT_Score\u0027], bins\u003d80)\nplt.title(\u0027Histogram of Values\u0027)\nplt.xlabel(\u0027ACT_Score\u0027)\nplt.ylabel(\u0027count\u0027)\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Finding outliers for COSTT4_A\n\nimport matplotlib.pyplot as plt\n\n# Convert PySpark DataFrame to Pandas DataFrame for visualization\ndf_pd \u003d df.select(\"COSTT4_A\").toPandas()\n\n# Plotting a histogram using Matplotlib\nplt.hist(df_pd[\u0027COSTT4_A\u0027], bins\u003d50)\nplt.title(\u0027Histogram of Values\u0027)\nplt.xlabel(\u0027COSTT4_A\u0027)\nplt.ylabel(\u0027count\u0027)\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Remove outliers and replace with NaN\ndfp \u003d dfp.withColumn(\u0027SAT_Score\u0027, when((col(\u0027SAT_Score\u0027) \u003c 700) | (col(\u0027SAT_Score\u0027) \u003e 1550), 0.0).otherwise(col(\u0027SAT_Score\u0027)))\ndfp \u003d dfp.withColumn(\u0027Average_SAT\u0027, when((col(\u0027Average_SAT\u0027) \u003c 1500) | (col(\u0027Average_SAT\u0027) \u003e 3100), 0.0).otherwise(col(\u0027Average_SAT\u0027)))\ndfp \u003d dfp.withColumn(\u0027ACT_Score\u0027, when((col(\u0027ACT_Score\u0027) \u003c 40) | (col(\u0027ACT_Score\u0027) \u003e 110), 0.0).otherwise(col(\u0027ACT_Score\u0027)))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#remove Null values converted to \u00270\u0027 for SAT_Score, Average_Score, ACT_Score, Age_Entry\ndfp \u003d dfp.filter(dfp[\"SAT_Score\"] !\u003d 0.0)\ndfp \u003d dfp.filter(dfp[\"Average_SAT\"] !\u003d 0.0)\ndfp \u003d dfp.filter(dfp[\"ACT_Score\"] !\u003d 0.0)\ndfp \u003d dfp.filter(dfp[\"Age_Entry\"] !\u003d 0.0)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Used StringIndexer to convert categorical features represented as strings into numerical features "
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml.feature import StringIndexer\n\nindexer \u003d StringIndexer(inputCol\u003d\"STABBR\", outputCol\u003d\"STABBRIndex\")\ndfp \u003d indexer.fit(dfp).transform(dfp)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Drop unwanted columns"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndfp \u003d dfp.drop(\u0027SATVRMID\u0027, \u0027SATMTMID\u0027 ,\u0027ACTCMMID\u0027, \u0027ACTENMID\u0027,\u0027ACTMTMID\u0027,\u0027SAT_AVG\u0027,\u0027SAT_AVG_ALL\u0027)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndfp.printSchema()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Creating a temporary table with the clean data"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntemp_table_name \u003d \"US_Scorecard\"\n\ndfp.createOrReplaceTempView(temp_table_name)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Selecting only the columns that will be used as features"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\u0027\u0027\u0027\u0027\nremoved fields that were null : NANTI, HBCU,PBI,ANNHI,TRIBAL,AANAPII,HSI,GT_25K_P6\n\u0027\u0027\u0027\n\ndfp_1\u003d dfp.select(\u0027STABBRIndex\u0027 , \u0027CONTROL\u0027, \u0027MD_EARN_WNE_P10\u0027, \u0027GRAD_DEBT_MDN_SUPP\u0027, \u0027SAT_Score\u0027, \u0027Average_SAT\u0027, \u0027ACT_Score\u0027, \u0027COSTT4_A\u0027,\u0027UGDS\u0027,\u0027TUITIONFEE_IN\u0027,\u0027TUITIONFEE_OUT\u0027,\u0027PCTPELL\u0027,\u0027LPSTAFFORD_CNT\u0027,\u0027LPSTAFFORD_AMT\u0027,\u0027LPPPLUS_CNT\u0027,\u0027LPPPLUS_AMT\u0027,\u0027BOOKSUPPLY\u0027,\u0027ROOMBOARD_ON\u0027,\u0027OTHEREXPENSE_ON\u0027,\u0027ROOMBOARD_OFF\u0027,\u0027OTHEREXPENSE_OFF\u0027,\u0027OTHEREXPENSE_FAM\u0027,\u0027ENDOWBEGIN\u0027,\u0027ENDOWEND\u0027,\u0027ADM_RATE\u0027,\u0027ENRL_ORIG_YR2_RT\u0027,\u0027UGDS_MEN\u0027,\u0027UGDS_WOMEN\u0027,\u0027AGE_ENTRY\u0027, \u0027RPY_3YR_RT_SUPP\u0027, col(\"Net_Price\").alias(\"label\"))\n\ndfp_1.show(2)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Splitting the data into training and testing data sets."
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nsplits \u003d dfp_1.randomSplit([0.7,0.3])\ntrain \u003d splits[0]\ntest \u003d splits[1].withColumnRenamed(\"label\", \"trueLabel\")\n\nprint (\"Training Rows:\", train.count(), \" Testing Rows:\", test.count())"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Assembler is a feature transformer that combines a given list of columns into a single vector column.\n### MinMax scaling method is a data normalization technique that rescales the features of a dataset so that they fall within a specified range, typically between 0 and 1.\n### We applied the above two transformer for the data set. "
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nassembler \u003d VectorAssembler(inputCols \u003d [\u0027STABBRIndex\u0027 , \u0027CONTROL\u0027,\u0027MD_EARN_WNE_P10\u0027, \u0027GRAD_DEBT_MDN_SUPP\u0027,\u0027RPY_3YR_RT_SUPP\u0027, \u0027SAT_Score\u0027, \u0027Average_SAT\u0027, \u0027ACT_Score\u0027,\u0027COSTT4_A\u0027,\u0027UGDS\u0027,\u0027TUITIONFEE_IN\u0027,\u0027TUITIONFEE_OUT\u0027,\u0027PCTPELL\u0027,\u0027LPSTAFFORD_CNT\u0027,\u0027LPSTAFFORD_AMT\u0027,\u0027LPPPLUS_CNT\u0027,\u0027LPPPLUS_AMT\u0027,\u0027BOOKSUPPLY\u0027,\u0027ROOMBOARD_ON\u0027,\u0027OTHEREXPENSE_ON\u0027,\u0027ROOMBOARD_OFF\u0027,\u0027OTHEREXPENSE_OFF\u0027,\u0027OTHEREXPENSE_FAM\u0027,\u0027ENDOWBEGIN\u0027,\u0027ENDOWEND\u0027,\u0027ADM_RATE\u0027,\u0027ENRL_ORIG_YR2_RT\u0027,\u0027UGDS_MEN\u0027,\u0027UGDS_WOMEN\u0027, \u0027AGE_ENTRY\u0027], outputCol\u003d\"features\")\n\n\nminMax \u003d MinMaxScaler(inputCol \u003d assembler.getOutputCol(), outputCol\u003d\"normFeatures\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Feature importance\n### Its a technique that evaluates the significance of different input features in predicting the target variable."
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nrf \u003d RandomForestRegressor(labelCol\u003d\"label\", featuresCol\u003d\"normFeatures\" ) #numTrees\u003d10\n \npipeline0_rf \u003d Pipeline(stages\u003d[assembler, minMax, rf])\n \nmodel \u003d pipeline0_rf.fit(train)\n \nrfModel \u003d model.stages[-1] \nprint(rfModel.toDebugString)\n \nimport pandas as pd\n \nfeatureImp \u003d pd.DataFrame(list(zip(assembler.getInputCols(), rfModel.featureImportances)),\ncolumns\u003d[\"normFeatures\", \"importance\"])\nfeatureImp.sort_values(by\u003d\"importance\", ascending\u003dFalse)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Random Forest"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nrf \u003d RandomForestRegressor(labelCol\u003d\"label\", featuresCol\u003d\"normFeatures\")"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nmodel \u003d []\npipeline \u003d []"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Parameter Tuning for Train Validation Split"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Train validator parameters\n\nparamGrid \u003d ParamGridBuilder() \\\n.addGrid(rf.maxDepth, [8, 10]) \\\n.addGrid(rf.numTrees, [12, 15]) \\\n.addGrid(rf.minInfoGain, [0.0]) \\\n.addGrid(rf.maxBins, [58, 60]) \\\n.build()\n\n# paramGrid \u003d ParamGridBuilder() \\\n#  .addGrid(rf.maxDepth, [3, 5]) \\\n#  .addGrid(rf.minInfoGain, [0.0]) \\\n#  .addGrid(rf.maxBins, [58,60]) \\\n#  .build()"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Start recording time\nstart_time \u003d time.time()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Training the model using Train-Validation Split and Cross-Validation methods."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Organized workflow using pipelines, which help in chaining multiple stages of transformations and model training."
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\npipeline.insert(0, Pipeline(stages\u003d[assembler, minMax,rf]))\n\ntv \u003d TrainValidationSplit(estimator\u003dpipeline[0], evaluator\u003dRegressionEvaluator(), estimatorParamMaps\u003dparamGrid, trainRatio\u003d0.8)\n\n# the first model\nmodel.insert(0, tv.fit(train))"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# End recording time\nend_time \u003d time.time()\n\n \n# Calculate the elapsed time\nexecution_time \u003d end_time - start_time\nprint(\"Random Forest Model execution time with TVS: {:.2f} seconds\".format(execution_time))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Parameter Tuning for Cross Validation"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Cross Validator parameters\n\nparamGridCV \u003d ParamGridBuilder() \\\n    .addGrid(rf.maxDepth, [8, 10]) \\\n    .addGrid(rf.numTrees, [12, 15]) \\\n    .addGrid(rf.minInfoGain, [0.0]) \\\n    .addGrid(rf.maxBins, [58, 60]) \\\n    .build()"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Start recording time\nstart_time \u003d time.time()"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\npipeline.insert(1, Pipeline(stages\u003d[assembler, minMax, rf]))\n\n# K\u003d3, 5\nK \u003d 3\ncv \u003d CrossValidator(estimator\u003dpipeline[1], evaluator\u003dRegressionEvaluator(), estimatorParamMaps\u003dparamGridCV, numFolds\u003dK)\n\n# the second model\nmodel.insert(1, cv.fit(train))"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# End recording time\nend_time \u003d time.time()\n\n \n# Calculate the elapsed time\nexecution_time \u003d end_time - start_time\nprint(\"Random Forest Model execution time with CV: {:.2f} seconds\".format(execution_time))"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Test the model\n# list prediction\nprediction \u003d [] \npredicted \u003d []\ni \u003d 0\nfor i in range(2):\n  prediction.insert(i, model[i].transform(test))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Examine the Predicted and Actual Values\ni\u003d0\nfor i in range(2):\n  predicted.insert(i, prediction[i].select(\"normFeatures\", \"prediction\", \"trueLabel\"))\n  predicted[i].show(20)\n  "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Retrieve the Root Mean Square Error (RMSE) for Random Forest\n## There are a number of metrics used to measure the variance between predicted and actual values. The root mean square error (RMSE) is measured in the same units as the prediced and actual values. Here the RMSE indicates the average difference of netprice between predicted and actual netprice values."
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Retrieve the Root Mean Square Error (RMSE)\ni\u003d0\nrmses \u003d []\nfor i in range(2):\n  evaluator \u003d RegressionEvaluator(labelCol\u003d\"trueLabel\", predictionCol\u003d\"prediction\", metricName\u003d\"rmse\")\n  rmse \u003d evaluator.evaluate(predicted[i])\n  rmses.insert(i, rmse)\n  print (\"Random Forest Model\")\n  print (\"Model \", i, \": \", \"Root Mean Square Error (RMSE):\", rmses[i])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## R2 for RandomForest Regression (RF)\n## R2 is reffered as coefficient of determination and statistical measure that indicates how well the independent variable(s) explain the variability of the dependent variable. R2 values range from 0 to 1, where 0 indicates that the independent variable(s) do not explain any of the variability of the dependent variable, and 1 indicates that they explain all of it.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Retrieve the R2\n\ni\u003d0\nr2s \u003d []\nfor i in range(2):\n  evaluator \u003d RegressionEvaluator(labelCol\u003d\"trueLabel\", predictionCol\u003d\"prediction\", metricName\u003d\"r2\")\n  r2 \u003d evaluator.evaluate(predicted[i])\n  r2s.insert(i, r2)\n  print (\"Model \", i, \": \", \"Coefficient of Determination (R2):\", r2s[i])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Checking if Model Overfitted"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Finding if the model is overfitted by checking the results with the train data\nprediction \u003d []\npredicted \u003d []\ni \u003d 0\nfor i in range(1):\n  prediction.insert(i, model[i].transform(train))\ni\u003d0\nfor i in range(1):\n  predicted.insert(i, prediction[i].select(\"normFeatures\", \"prediction\", \"label\"))\n \ni\u003d0\nrmses \u003d []\nfor i in range(1):\n  evaluator \u003d RegressionEvaluator(labelCol\u003d\"label\", predictionCol\u003d\"prediction\", metricName\u003d\"rmse\")\n  rmse \u003d evaluator.evaluate(predicted[i])\n  rmses.insert(i, rmse)\n  print (\"RF Model(Train data) \", i, \": \", \"Root Mean Square Error (RMSE):\", rmses[i])\n \n# Retrieve the R2 for train data\n \ni\u003d0\nr2s \u003d []\nfor i in range(1):\n  evaluator \u003d RegressionEvaluator(labelCol\u003d\"label\", predictionCol\u003d\"prediction\", metricName\u003d\"r2\")\n  r2 \u003d evaluator.evaluate(predicted[i])\n  r2s.insert(i, r2)\n  print (\"RF Model(Train data) \", i, \": \", \"Coefficient of Determination (R2):\", r2s[i])\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Gradient Boost Trees\n"
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ngbt \u003d GBTRegressor(labelCol\u003d\"label\", featuresCol\u003d\"normFeatures\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nmodel \u003d []\npipeline \u003d []"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Parameter Tuning for Train-Validation split\n"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Train Validation Parameters\n\nparamGrid \u003d ParamGridBuilder()\\\n.addGrid(gbt.maxDepth, [5, 10, 20])\\\n.addGrid(gbt.maxBins, [52, 55]) \\\n.addGrid(gbt.maxIter, [10,20,30]) \\\n.build()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Start recording time\nstart_time \u003d time.time()"
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\npipeline.insert(0, Pipeline(stages\u003d[assembler, minMax, gbt]))\n\ntv \u003d TrainValidationSplit(estimator\u003dpipeline[0], evaluator\u003dRegressionEvaluator(), estimatorParamMaps\u003dparamGrid, trainRatio\u003d0.8)\n\n# the first model\nmodel.insert(0, tv.fit(train))"
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# End recording time\nend_time \u003d time.time()\n\n \n# Calculate the elapsed time\nexecution_time \u003d end_time - start_time\nprint(\"Gradient Boost Trees Model execution time with TVS: {:.2f} seconds\".format(execution_time))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Parameter Tuning for Cross Validation"
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Cross Validation Parameters\n\nparamGridCV \u003d ParamGridBuilder()\\\n.addGrid(gbt.maxDepth, [5, 10, 20])\\\n.addGrid(gbt.maxBins, [52, 55]) \\\n.addGrid(gbt.maxIter, [10,20,30]) \\\n.build()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Start recording time\nstart_time \u003d time.time()"
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\npipeline.insert(1, Pipeline(stages\u003d[assembler, minMax, gbt]))\n\n# K\u003d3, 5\nK \u003d 3\ncv \u003d CrossValidator(estimator\u003dpipeline[1], evaluator\u003dRegressionEvaluator(), estimatorParamMaps\u003dparamGridCV, numFolds\u003dK)\n\n# the second model\nmodel.insert(1, cv.fit(train))"
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# End recording time\nend_time \u003d time.time()\n\n \n# Calculate the elapsed time\nexecution_time \u003d end_time - start_time\nprint(\"Gradient Boost Trees Model execution time with CV: {:.2f} seconds\".format(execution_time))"
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Test the model\n# list prediction\nprediction \u003d [] \npredicted \u003d []\ni \u003d 0\nfor i in range(2):\n  prediction.insert(i, model[i].transform(test))"
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Examine the Predicted and Actual Values\ni\u003d0\nfor i in range(2):\n  predicted.insert(i, prediction[i].select(\"normFeatures\", \"prediction\", \"trueLabel\"))\n  predicted[i].show(20)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## RMSE for Gradient Boost Trees Model\n"
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Retrieve the Root Mean Square Error (RMSE)\ni\u003d0\nrmses \u003d []\nfor i in range(2):\n  evaluator \u003d RegressionEvaluator(labelCol\u003d\"trueLabel\", predictionCol\u003d\"prediction\", metricName\u003d\"rmse\")\n  rmse \u003d evaluator.evaluate(predicted[i])\n  rmses.insert(i, rmse)\n  print (\"Gradient Boost Trees Model\")\n  print (\"Model \", i, \": \", \"Root Mean Square Error (RMSE):\", rmses[i])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## R2 for Gradient Boost Trees Model"
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Retrieve the R2\ni\u003d0\nr2s \u003d []\nfor i in range(2):\n  evaluator \u003d RegressionEvaluator(labelCol\u003d\"trueLabel\", predictionCol\u003d\"prediction\", metricName\u003d\"r2\")\n  r2 \u003d evaluator.evaluate(predicted[i])\n  r2s.insert(i, r2)\n  print (\"Model \", i, \": \", \"Coefficient of Determination (R2):\", r2s[i])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Decision Tree Regression"
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndt \u003d DecisionTreeRegressor(labelCol\u003d\"label\", featuresCol\u003d\"normFeatures\")"
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nmodel \u003d []\npipeline \u003d []"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Parameter Tuning Train-Validation split"
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Train validator parameters\n\nparamGrid \u003d ParamGridBuilder() \\\n .addGrid(dt.maxDepth, [15, 20]) \\\n .addGrid(dt.minInfoGain, [0.0]) \\\n .addGrid(dt.maxBins, [58,60]) \\\n .build()\n\n# paramGrid \u003d ParamGridBuilder() \\\n#  .addGrid(dt.maxDepth, [5, 10]) \\\n#  .addGrid(dt.minInfoGain, [0.0]) \\\n#  .addGrid(dt.maxBins, [58,60]) \\\n#  .build()"
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Start recording time\nstart_time \u003d time.time()"
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\npipeline.insert(0, Pipeline(stages\u003d[assembler,minMax, dt]))\n\ntv \u003d TrainValidationSplit(estimator\u003dpipeline[0], evaluator\u003dRegressionEvaluator(), estimatorParamMaps\u003dparamGrid, trainRatio\u003d0.8)\n\n# the first model\nmodel.insert(0, tv.fit(train))"
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# End recording time\nend_time \u003d time.time()\n\n \n# Calculate the elapsed time\nexecution_time \u003d end_time - start_time\nprint(\"Decision Tree Regression Model execution time with TVS: {:.2f} seconds\".format(execution_time))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Parameter Tuning for Cross Validation split\n"
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Cross Validator parameters\n\nparamGridCV \u003d ParamGridBuilder() \\\n .addGrid(dt.maxDepth, [15, 20]) \\\n .addGrid(dt.minInfoGain, [0.0]) \\\n .addGrid(dt.maxBins, [58,60]) \\\n .build()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Start recording time\nstart_time \u003d time.time()"
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\npipeline.insert(1, Pipeline(stages\u003d[assembler, minMax, dt]))\n\n# K\u003d3, 5\nK \u003d 3\ncv \u003d CrossValidator(estimator\u003dpipeline[1], evaluator\u003dRegressionEvaluator(), estimatorParamMaps\u003dparamGridCV, numFolds\u003dK)\n\n# the second model\nmodel.insert(1, cv.fit(train))"
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# End recording time\nend_time \u003d time.time()\n\n \n# Calculate the elapsed time\nexecution_time \u003d end_time - start_time\nprint(\"Decision Tree Regression Model execution time with CV: {:.2f} seconds\".format(execution_time))"
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Test the model\n# list prediction\nprediction \u003d [] \npredicted \u003d []\ni \u003d 0\nfor i in range(2):\n  prediction.insert(i, model[i].transform(test))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Examine the Predicted and Actual Values\ni\u003d0\nfor i in range(2):\n  predicted.insert(i, prediction[i].select(\"normFeatures\", \"prediction\", \"trueLabel\"))\n  predicted[i].show(20)\n  "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## RMSE for Decision Tree Regression Model"
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Retrieve the Root Mean Square Error (RMSE)\ni\u003d0\nrmses \u003d []\nfor i in range(2):\n  evaluator \u003d RegressionEvaluator(labelCol\u003d\"trueLabel\", predictionCol\u003d\"prediction\", metricName\u003d\"rmse\")\n  rmse \u003d evaluator.evaluate(predicted[i])\n  rmses.insert(i, rmse)\n  print (\"Decision Tree Regression Model\")\n  print (\"Model \", i, \": \", \"Root Mean Square Error (RMSE):\", rmses[i])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## R2 for Decision Tree Regression Model\n"
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Retrieve the R2\n\ni\u003d0\nr2s \u003d []\nfor i in range(2):\n  evaluator \u003d RegressionEvaluator(labelCol\u003d\"trueLabel\", predictionCol\u003d\"prediction\", metricName\u003d\"r2\")\n  r2 \u003d evaluator.evaluate(predicted[i])\n  r2s.insert(i, r2)\n  print (\"Model \", i, \": \", \"Coefficient of Determination (R2):\", r2s[i])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Linear Regression"
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nlr \u003d LinearRegression(labelCol\u003d\"label\", featuresCol\u003d\"normFeatures\")"
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nmodel \u003d []\npipeline \u003d []"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Parameter Tuning for Train-Validation split"
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Train validator parameters\n\n\n\nparamGrid \u003d ParamGridBuilder() \\\n.addGrid(lr.maxIter, [20,30,40]) \\\n.addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n.addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n.addGrid(lr.standardization, [True, False]) \\\n.build()"
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Start recording time\nstart_time \u003d time.time()"
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\npipeline.insert(0, Pipeline(stages\u003d[assembler, minMax, lr]))\n\ntv \u003d TrainValidationSplit(estimator\u003dpipeline[0], evaluator\u003dRegressionEvaluator(), estimatorParamMaps\u003dparamGrid, trainRatio\u003d0.8)\n\n# the first model\nmodel.insert(0, tv.fit(train))"
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# End recording time\nend_time \u003d time.time()\n\n \n# Calculate the elapsed time\nexecution_time \u003d end_time - start_time\nprint(\"Linear Regression Model execution time with TVS: {:.2f} seconds\".format(execution_time))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Parameter Tuning for Cross Validation"
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Cross Validator parameters\n\nparamGridCV \u003d ParamGridBuilder() \\\n.addGrid(lr.maxIter, [20,30,40]) \\\n.addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n.addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n.addGrid(lr.standardization, [True, False]) \\\n.build()"
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Start recording time\nstart_time \u003d time.time()"
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\npipeline.insert(1, Pipeline(stages\u003d[assembler, minMax, lr]))\n\n# K\u003d3, 5\nK \u003d 3\ncv \u003d CrossValidator(estimator\u003dpipeline[1], evaluator\u003dRegressionEvaluator(), estimatorParamMaps\u003dparamGridCV, numFolds\u003dK)\n\n# the second model\nmodel.insert(1, cv.fit(train))"
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# End recording time\nend_time \u003d time.time()\n\n \n# Calculate the elapsed time\nexecution_time \u003d end_time - start_time\nprint(\"Linear Regression Model execution time with CV: {:.2f} seconds\".format(execution_time))"
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Test the model\n# list prediction\nprediction \u003d [] \npredicted \u003d []\ni \u003d 0\nfor i in range(2):\n  prediction.insert(i, model[i].transform(test))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Examine the Predicted and Actual Values\ni\u003d0\nfor i in range(2):\n  predicted.insert(i, prediction[i].select(\"normFeatures\", \"prediction\", \"trueLabel\"))\n  predicted[i].show(20)\n  "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## RMSE for Linear Regression Model"
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Retrieve the Root Mean Square Error (RMSE)\ni\u003d0\nrmses \u003d []\nfor i in range(2):\n  evaluator \u003d RegressionEvaluator(labelCol\u003d\"trueLabel\", predictionCol\u003d\"prediction\", metricName\u003d\"rmse\")\n  rmse \u003d evaluator.evaluate(predicted[i])\n  rmses.insert(i, rmse)\n  print (\"Linear Regression Model\")\n  print (\"Model \", i, \": \", \"Root Mean Square Error (RMSE):\", rmses[i])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## R2 for Linear Regression Model"
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Retrieve the R2\n\ni\u003d0\nr2s \u003d []\nfor i in range(2):\n  evaluator \u003d RegressionEvaluator(labelCol\u003d\"trueLabel\", predictionCol\u003d\"prediction\", metricName\u003d\"r2\")\n  r2 \u003d evaluator.evaluate(predicted[i])\n  r2s.insert(i, r2)\n  print (\"Model \", i, \": \", \"Coefficient of Determination (R2):\", r2s[i])"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}